# Local Document QA with LLM (Mistral via Ollama)

This project demonstrates a simple yet powerful pipeline to perform question-answering over a local PDF document using a local LLM (Mistral) via [Ollama](https://ollama.com). It also supports switching to OpenAI's API if desired.

---

## ğŸ”§ Features

- Load and chunk PDF documents
- Generate embeddings using `OllamaEmbeddings` (Mistral)
- Store/retrieve chunks with FAISS vectorstore
- Run local LLM (`mistral`) for question-answering using LangChain
- Automatically starts Ollama server if not already running
- Supports `ChatOllama` and `OpenAIEmbeddings` (if key provided)

---

## ğŸ§  Requirements

- macOS with Apple Silicon (tested on M1 Max)
- Python 3.11+
- [Ollama](https://ollama.com) installed (`brew install ollama`)
- Mistral model pulled: `ollama pull mistral`

---

## ğŸ“ Project Structure
```
doc-qa-llm/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ nature_paper.pdf       # Your input PDF
â”‚   â””â”€â”€ faiss_index/           # Saved vectorstore
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ process_document.py    # Main logic
â”œâ”€â”€ .env                       # Contains your OPENAI_API_KEY (optional)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
```
---

## â–¶ï¸ Running the Pipeline

1. **Clone repo** and install deps:
   ```bash
   python -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt

2. **Pull the mistral model:**
   ollama pull mistral

3. **Run the script:**
   python scripts/process_document.py

--- 

âœ… Output

The script will:
	â€¢	Start Ollama if not running
	â€¢	Load and chunk your PDF
	â€¢	Embed chunks with Mistral
	â€¢	Store vectors in FAISS
	â€¢	Run a sample QA:
â€œWhat is the main hypothesis of the paper?â€

â¸»

ğŸŒ Optional: Use OpenAI API Instead

To use OpenAI instead of local LLM:
	â€¢	Install openai and langchain-openai
	â€¢	Replace OllamaEmbeddings/ChatOllama with OpenAIEmbeddings and ChatOpenAI
	â€¢	Make sure .env has your OPENAI_API_KEY

â¸»

ğŸ–¥ï¸ Next Steps
	â€¢	Add a Streamlit or FastAPI frontend
	â€¢	Allow uploading different PDFs dynamically
	â€¢	Build a UI for interactive chat with context

â¸»

ğŸ“Œ Notes
	â€¢	You do not need an internet connection to run the Mistral model after pulling it once
	â€¢	This is a great example of using agentic AI tools locally
